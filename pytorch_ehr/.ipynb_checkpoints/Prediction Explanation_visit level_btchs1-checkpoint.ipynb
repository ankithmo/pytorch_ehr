{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import basic requirements\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import pickle as pk\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load our EHR model related files\n",
    "import models as model \n",
    "from EHRDataloader import EHRdataFromPickles, EHRdataloader  \n",
    "import utils as ut \n",
    "from EHREmb import EHREmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just using original code for now\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load our Pretrained model\n",
    "\n",
    "best_model = torch.load('../models/hf.trainEHRmodel1.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EHR_RNN(\n",
       "  (embed): Embedding(30000, 128, padding_idx=0)\n",
       "  (rnn_c): GRU(128, 64, batch_first=True, dropout=0.1)\n",
       "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embed.weight',\n",
       "              tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "                        0.0000e+00,  0.0000e+00],\n",
       "                      [ 3.8867e-01, -5.0994e-01, -4.2380e-01,  ...,  1.4341e-01,\n",
       "                       -2.1141e-02,  1.8216e-01],\n",
       "                      [ 2.1339e-01,  1.0675e-01,  6.4853e-01,  ...,  1.6804e-01,\n",
       "                       -3.0365e-01,  3.8755e-01],\n",
       "                      ...,\n",
       "                      [ 2.1458e-10, -6.2775e-02, -6.7921e-42,  ...,  3.4711e-04,\n",
       "                        7.6896e-01, -7.1290e-39],\n",
       "                      [ 4.8890e-03, -5.9261e-42, -1.5059e-04,  ..., -3.4045e-04,\n",
       "                       -2.1071e-03, -1.3142e-36],\n",
       "                      [ 4.1841e-02, -1.9407e-26, -1.2908e-24,  ..., -2.0760e-02,\n",
       "                       -7.4471e-02,  1.7137e-03]], device='cuda:0')),\n",
       "             ('rnn_c.weight_ih_l0',\n",
       "              tensor([[-0.0331, -0.0380, -0.0410,  ...,  0.1258, -0.0227,  0.0188],\n",
       "                      [-0.0580,  0.0781, -0.0404,  ...,  0.0485,  0.0243,  0.0082],\n",
       "                      [ 0.0322, -0.0092,  0.1520,  ..., -0.0726,  0.0109, -0.0561],\n",
       "                      ...,\n",
       "                      [ 0.1618, -0.0528, -0.0053,  ..., -0.0425,  0.1031, -0.0195],\n",
       "                      [-0.1484, -0.0111, -0.0190,  ..., -0.0360,  0.0210, -0.1162],\n",
       "                      [ 0.0615,  0.0517,  0.0768,  ...,  0.0938, -0.1684, -0.0571]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn_c.weight_hh_l0',\n",
       "              tensor([[-0.0358,  0.0738,  0.0216,  ..., -0.0607,  0.0776, -0.0216],\n",
       "                      [ 0.0131, -0.0257,  0.0537,  ...,  0.0349,  0.0748,  0.0272],\n",
       "                      [ 0.0055, -0.0534, -0.0066,  ..., -0.0175, -0.0337,  0.0518],\n",
       "                      ...,\n",
       "                      [ 0.0242, -0.1272, -0.0081,  ...,  0.1477, -0.0732,  0.1048],\n",
       "                      [-0.0344,  0.0549,  0.0877,  ..., -0.0284,  0.1405,  0.1005],\n",
       "                      [ 0.0027, -0.0212,  0.1462,  ...,  0.0640, -0.0201,  0.2071]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn_c.bias_ih_l0',\n",
       "              tensor([ 0.0752,  0.0826,  0.0475,  0.0424,  0.0607,  0.1448,  0.0766,  0.0776,\n",
       "                       0.0381,  0.1284,  0.0654,  0.0746, -0.0037, -0.0030,  0.0520,  0.0046,\n",
       "                       0.1024,  0.0365,  0.0475,  0.0649,  0.0328,  0.0946,  0.0317,  0.0614,\n",
       "                       0.0713, -0.0225,  0.0274,  0.0630,  0.0613,  0.1386,  0.0764, -0.0052,\n",
       "                       0.0773, -0.0018,  0.0502,  0.0118, -0.0175, -0.0081,  0.0313,  0.0085,\n",
       "                       0.1015,  0.0179,  0.0402,  0.1592,  0.0048, -0.0126,  0.0643,  0.0065,\n",
       "                       0.1232,  0.0624,  0.0138,  0.1142,  0.0024, -0.0013,  0.0288, -0.0144,\n",
       "                       0.0857,  0.0759,  0.0526,  0.0060,  0.0362,  0.0265,  0.0265,  0.0140,\n",
       "                      -0.1923, -0.1033,  0.0246,  0.0054,  0.0024,  0.0089, -0.0125, -0.0447,\n",
       "                       0.0238, -0.0420,  0.0263, -0.0918,  0.1110, -0.0995, -0.0171,  0.0804,\n",
       "                      -0.0324,  0.1364, -0.0679, -0.0487, -0.1290,  0.0688, -0.0370,  0.0056,\n",
       "                      -0.0390,  0.0092, -0.0055, -0.1703,  0.1128, -0.0528, -0.0890,  0.0690,\n",
       "                      -0.0449, -0.0574,  0.1362,  0.0943,  0.0806,  0.0031,  0.2327,  0.1575,\n",
       "                      -0.1675,  0.0089, -0.1435, -0.0798,  0.0008, -0.0281,  0.0245,  0.0075,\n",
       "                      -0.1691,  0.0073, -0.0982, -0.1270,  0.0199,  0.1379, -0.0346,  0.1975,\n",
       "                       0.0384, -0.0446, -0.1101,  0.0164, -0.0228, -0.0108,  0.0474,  0.2098,\n",
       "                       0.0280,  0.0056, -0.0118,  0.1392, -0.1007,  0.0458, -0.0885,  0.0626,\n",
       "                      -0.0327,  0.0600, -0.0390, -0.0150, -0.0514,  0.1004, -0.0320,  0.0689,\n",
       "                       0.0654, -0.1008, -0.1010,  0.1075,  0.0520,  0.0161, -0.1058,  0.0782,\n",
       "                       0.0663, -0.0883, -0.1419, -0.0284,  0.0638, -0.0763,  0.0156,  0.0586,\n",
       "                      -0.0324, -0.0304, -0.0216, -0.0449, -0.0850, -0.0777,  0.0070,  0.1178,\n",
       "                      -0.0529, -0.0223, -0.0262, -0.1105, -0.0159, -0.0208,  0.0422,  0.0708,\n",
       "                       0.0457, -0.1173, -0.0405,  0.0932,  0.0123,  0.0133, -0.1055, -0.1123,\n",
       "                       0.0209,  0.0918, -0.0504, -0.0853, -0.0516, -0.0322,  0.1048, -0.0157],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn_c.bias_hh_l0',\n",
       "              tensor([ 0.1436,  0.1681,  0.0168,  0.0501,  0.0756,  0.1305, -0.0004,  0.0831,\n",
       "                       0.0258,  0.0581,  0.0506,  0.0428, -0.0109,  0.1257, -0.0486,  0.0884,\n",
       "                       0.0107,  0.0593,  0.0611,  0.0376,  0.0837,  0.0993,  0.0345,  0.0202,\n",
       "                       0.0553, -0.0843,  0.0352,  0.0787, -0.0196,  0.1309,  0.0926,  0.0146,\n",
       "                       0.0760,  0.0752,  0.0691,  0.0922,  0.0270,  0.0624,  0.0286,  0.0673,\n",
       "                       0.0745,  0.0686,  0.0274,  0.1101,  0.0600,  0.0133,  0.0375,  0.0613,\n",
       "                       0.1507,  0.0739,  0.1156,  0.0960, -0.0158, -0.0567,  0.0171, -0.0235,\n",
       "                       0.0851,  0.0717,  0.0852,  0.0296,  0.0592,  0.1073,  0.0226,  0.0316,\n",
       "                      -0.0623, -0.1031,  0.0125, -0.0048,  0.0183,  0.0177, -0.0349, -0.1000,\n",
       "                       0.0152, -0.1574,  0.0952, -0.2158,  0.0462, -0.0559,  0.0331,  0.0541,\n",
       "                       0.0286,  0.1067, -0.0399, -0.0936, -0.1546,  0.0035,  0.0890,  0.0010,\n",
       "                      -0.0241,  0.0339, -0.0447, -0.0774,  0.0716, -0.1115,  0.0180,  0.0674,\n",
       "                      -0.1059, -0.0651,  0.1582,  0.0609, -0.0234,  0.0760,  0.1784,  0.1553,\n",
       "                      -0.0797,  0.0713, -0.0896, -0.0108, -0.0549, -0.0194,  0.1225, -0.0756,\n",
       "                      -0.2026, -0.0270, -0.0500, -0.1043,  0.0024,  0.1307,  0.0015,  0.1132,\n",
       "                       0.0250, -0.0704, -0.0458,  0.0880, -0.1049, -0.0054,  0.0326,  0.1446,\n",
       "                       0.0349, -0.0449,  0.0826,  0.0725, -0.0648,  0.0074, -0.1148, -0.0312,\n",
       "                      -0.0464, -0.0044, -0.0501, -0.1123, -0.0672,  0.0456,  0.0180,  0.0696,\n",
       "                       0.0372,  0.0378, -0.0860,  0.0750,  0.1101, -0.0440,  0.0989, -0.1002,\n",
       "                      -0.0712, -0.0908, -0.1404, -0.0356,  0.0540, -0.0422,  0.0709, -0.0475,\n",
       "                      -0.0450,  0.0519,  0.0990, -0.1001,  0.0171, -0.1318, -0.0753,  0.0255,\n",
       "                       0.0249,  0.0286,  0.1058, -0.0950, -0.0360,  0.0751,  0.0322,  0.0080,\n",
       "                       0.1392, -0.0077, -0.1019, -0.0166,  0.0219, -0.1522,  0.0435,  0.1096,\n",
       "                      -0.0020,  0.0626, -0.0602,  0.0013,  0.0710,  0.0343,  0.0547, -0.0220],\n",
       "                     device='cuda:0')),\n",
       "             ('out.weight',\n",
       "              tensor([[ 0.2667, -0.1561,  0.2130,  0.2268, -0.2192,  0.2999,  0.1084, -0.2089,\n",
       "                        0.0346,  0.2598,  0.1006, -0.2424,  0.1568,  0.2216,  0.2130, -0.3469,\n",
       "                        0.0385,  0.0018, -0.1380, -0.1445,  0.2667, -0.1477,  0.2685,  0.1304,\n",
       "                       -0.2585,  0.2723, -0.1738,  0.2589,  0.2142,  0.3117, -0.2334,  0.2019,\n",
       "                       -0.2081,  0.2025, -0.1393,  0.2985,  0.2262,  0.2368, -0.1888, -0.1047,\n",
       "                       -0.1982, -0.3008, -0.2199,  0.1954, -0.2084, -0.2225,  0.2151, -0.2563,\n",
       "                        0.1833,  0.2191,  0.2474, -0.1983,  0.2129,  0.2177, -0.1435,  0.0831,\n",
       "                        0.2135,  0.2352, -0.2028, -0.2521, -0.2364,  0.1114,  0.0772,  0.1400]],\n",
       "                     device='cuda:0')),\n",
       "             ('out.bias', tensor([-0.0294], device='cuda:0'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:15<00:00,  7.69it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = EHRdataFromPickles(root_dir = '../data/', \n",
    "                              file = '/dhf_test_60Kb_cid_cscl1.combined.valid', \n",
    "                              sort= True,\n",
    "                              model='RNN')\n",
    "mbs_list = list(tqdm(EHRdataloader(ds, batch_size=100, packPadMode=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the token dict\n",
    "tk_dc= pk.load(open('../data/dhf_test_60Kb_cid_cscl1.types','rb'))\n",
    "diag_t= pd.DataFrame.from_dict(tk_dc,orient='index').reset_index()\n",
    "diag_t.columns=['DIAGNOSIS_ID','TOKEN_ID']\n",
    "diag=pd.read_csv('../data/HF_D_DIAGNOSIS', sep='|')\n",
    "diag_tk=pd.merge(diag_t, diag, how='left', on='DIAGNOSIS_ID',sort=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DIAGNOSIS_ID</th>\n",
       "      <th>TOKEN_ID</th>\n",
       "      <th>DIAGNOSIS_TYPE</th>\n",
       "      <th>DIAGNOSIS_CODE</th>\n",
       "      <th>DIAGNOSIS_DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>19281</td>\n",
       "      <td>ICD9</td>\n",
       "      <td>001.1</td>\n",
       "      <td>CHOLERA DUE TO VIBRIO CHOLERAE EL TOR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DIAGNOSIS_ID  TOKEN_ID DIAGNOSIS_TYPE DIAGNOSIS_CODE  \\\n",
       "0             3     19281           ICD9          001.1   \n",
       "\n",
       "                   DIAGNOSIS_DESCRIPTION  \n",
       "0  CHOLERA DUE TO VIBRIO CHOLERAE EL TOR  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag_tk[diag_tk['DIAGNOSIS_ID']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribution for the prediction\n",
    "weights = best_model.rnn_c.state_dict()\n",
    "_, W_iz, _ = np.split(weights['weight_ih_l0'].cpu().numpy(), 3, 0) ## to get the weights of the middle gate\n",
    "_, W_hz, _ = np.split(weights['weight_hh_l0'].cpu().numpy(), 3, 0)\n",
    "_, b_z, _ = np.split(weights['bias_ih_l0'].cpu().numpy() + weights['bias_hh_l0'].cpu().numpy(), 3)\n",
    "weights_linear = best_model.out.state_dict()\n",
    "W = weights_linear['weight'].cpu().numpy()\n",
    "b = weights_linear['bias'].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26667246, -0.1561053 ,  0.21302247,  0.22679123, -0.21923877,\n",
       "        0.29989025,  0.10841151, -0.20886146,  0.03460821,  0.2597521 ,\n",
       "        0.10061447, -0.24237964,  0.15679884,  0.2215598 ,  0.21295518,\n",
       "       -0.3469017 ,  0.03846961,  0.00176884, -0.13798143, -0.14447302,\n",
       "        0.2666596 , -0.14771777,  0.2685269 ,  0.13044381, -0.25845376,\n",
       "        0.2722527 , -0.17379832,  0.25891566,  0.21417032,  0.31167227,\n",
       "       -0.23342112,  0.2018602 , -0.20814937,  0.20248793, -0.13926002,\n",
       "        0.29849377,  0.22623911,  0.23675893, -0.1887609 , -0.10474794,\n",
       "       -0.19818154, -0.30080616, -0.21992539,  0.19540717, -0.20844392,\n",
       "       -0.22251134,  0.21512419, -0.25633875,  0.18334119,  0.21913089,\n",
       "        0.24736837, -0.19827954,  0.21285443,  0.21771058, -0.14350338,\n",
       "        0.08306476,  0.21354948,  0.23521651, -0.2028091 , -0.25210553,\n",
       "       -0.23640904,  0.1114354 ,  0.07720751,  0.13997132], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mb 0\n",
      "outshape torch.Size([100, 4, 64])\n",
      "Patient 1 label:  tensor([[1.]]) \n",
      "pred_score : tensor(0.9759)\n",
      "\n",
      "Explanation score per visit:  [-0.019679716, 0.2769947, 0.25691563, 0.761021]\n",
      "\n",
      "patient visits: \n",
      "Visit: 1  with Explanation Score: -0.019679716 [370, 407, 654, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [[654, 370, 407], ['OTHER LEFT BUNDLE BRANCH BLOCK', 'OTHER MALAISE AND FATIGUE', 'OTHER DYSPNEA AND RESPIRATORY ABNORMALITY']] \n",
      "\n",
      "Visit: 2  with Explanation Score: 0.2769947 [654, 957, 697, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [[654, 957, 697, 21], ['OTHER LEFT BUNDLE BRANCH BLOCK', 'SWELLING OF LIMB', 'EDEMA', 'UNSPECIFIED ESSENTIAL HYPERTENSION']] \n",
      "\n",
      "Visit: 3  with Explanation Score: 0.25691563 [6279, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [[6279], ['ASBESTOSIS']] \n",
      "\n",
      "Visit: 4  with Explanation Score: 0.761021 [975, 192, 5, 8, 22, 251, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [[5, 8, 22, 251, 975, 192], ['Diabetes mellitus without mention of complication, type II or unspecified type, not stated as uncontrolled', 'OTHER AND UNSPECIFIED HYPERLIPIDEMIA', 'CORONARY ATHEROSCLEROSIS OF NATIVE CORONARY ARTERY', 'OTHER SPECIFIED CARDIAC DYSRHYTHMIAS', 'HEART VALVE REPLACED BY OTHER MEANS', 'POSTSURGICAL AORTOCORONARY BYPASS STATUS']] \n",
      "\n",
      "\u001b[1;31mThe above looks acceptable\u001b[0m\n",
      "Patient -1 label:  tensor([[0.]]) \n",
      "pred_score : tensor(0.6973)\n",
      "\n",
      "Explanation score per visit:  [-0.0065328702, -0.04687721, 0.43944335, 0.12596482]\n",
      "\n",
      "patient visits: \n",
      "Visit: 1  with Explanation Score: -0.0065328702 [702, 69, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [[69, 702], ['UNSPECIFIED CHEST PAIN', 'OTHER NONSPECIFIC ABNORMAL FUNCTION STUDY OF CARDIOVASCULAR SYSTEM']] \n",
      "\n",
      "Visit: 2  with Explanation Score: -0.04687721 [21, 8, 5, 358, 192, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [[358, 5, 8, 21, 192], ['EPISTAXIS', 'Diabetes mellitus without mention of complication, type II or unspecified type, not stated as uncontrolled', 'OTHER AND UNSPECIFIED HYPERLIPIDEMIA', 'UNSPECIFIED ESSENTIAL HYPERTENSION', 'POSTSURGICAL AORTOCORONARY BYPASS STATUS']] \n",
      "\n",
      "Visit: 3  with Explanation Score: 0.43944335 [1917, 2, 5, 192, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [[5, 21, 2, 1917, 192], ['Diabetes mellitus without mention of complication, type II or unspecified type, not stated as uncontrolled', 'UNSPECIFIED ESSENTIAL HYPERTENSION', 'CORONARY ATHEROSCLEROSIS OF UNSPECIFIED TYPE OF VESSEL, NATIVE OR GRAFT', 'CHRONIC ULCER OF OTHER SPECIFIED SITES', 'POSTSURGICAL AORTOCORONARY BYPASS STATUS']] \n",
      "\n",
      "Visit: 4  with Explanation Score: 0.12596482 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [[], []] \n",
      "\n",
      "\u001b[1;31mHaving a score for empty visit is NOT acceptable\u001b[0m \n",
      "\n",
      "\n",
      "mb 1\n",
      "outshape torch.Size([100, 3, 64])\n",
      "Patient 1 label:  tensor([[1.]]) \n",
      "pred_score : tensor(0.9307)\n",
      "\n",
      "Explanation score per visit:  [-0.015869766, 0.78207994, 0.4340796]\n",
      "\n",
      "patient visits: \n",
      "Visit: 1  with Explanation Score: -0.015869766 [235, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [[235], ['PNEUMONIA, ORGANISM UNSPECIFIED']] \n",
      "\n",
      "Visit: 2  with Explanation Score: 0.78207994 [370, 2, 21, 10915, 5121, 48, 340, 8, 5, 2263, 2009, 55, 241, 36, 765, 2349, 45, 795, 793, 6103, 28, 3402, 69, 4232, 491, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [[28, 45, 2009, 2263, 5, 8, 340, 48, 5121, 10915, 21, 2, 55, 491, 370, 69, 3402, 6103, 793, 795, 2349, 765, 36, 4232, 241], ['ATRIAL FIBRILLATION', 'Acute Kidney Failure, Unspecified', 'OTHER ALTERATION OF CONSCIOUSNESS', 'ACCIDENTAL POISONING BY UNSPECIFIED SEDATIVE OR HYPNOTIC', 'Diabetes mellitus without mention of complication, type II or unspecified type, not stated as uncontrolled', 'OTHER AND UNSPECIFIED HYPERLIPIDEMIA', 'HYPOSMOLALITY AND/OR HYPONATREMIA', 'ANEMIA, UNSPECIFIED', 'OPIOID TYPE DEPENDENCE, CONTINUOUS USE', 'UNSPECIFIED DRUG DEPENDENCE, CONTINUOUS USE', 'UNSPECIFIED ESSENTIAL HYPERTENSION', 'CORONARY ATHEROSCLEROSIS OF UNSPECIFIED TYPE OF VESSEL, NATIVE OR GRAFT', 'CHRONIC AIRWAY OBSTRUCTION, NOT ELSEWHERE CLASSIFIED', 'CHRONIC RESPIRATORY FAILURE', 'OTHER MALAISE AND FATIGUE', 'UNSPECIFIED CHEST PAIN', 'OTHER NONSPECIFIC FINDINGS ON EXAMINATION OF BLOOD', 'POISONING BY UNSPECIFIED SEDATIVE OR HYPNOTIC', 'POISONING BY BENZODIAZEPINE-BASED TRANQUILIZERS', 'ACCIDENTAL POISONING BY BENZODIAZEPINE-BASED TRANQUILIZERS', 'Other encephalopathy', 'Rhabdomyolysis', 'Dehydration', 'Personal history of other diseases of respiratory system', 'Altered mental status']] \n",
      "\n",
      "Visit: 3  with Explanation Score: 0.4340796 [1199, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [[1199], ['UNSPECIFIED DEFICIENCY ANEMIA']] \n",
      "\n",
      "\u001b[1;31mThe above looks acceptable\u001b[0m\n",
      "Patient -1 label:  tensor([[1.]]) \n",
      "pred_score : tensor(0.7575)\n",
      "\n",
      "Explanation score per visit:  [-0.031054083, 0.1121871, -0.070365086]\n",
      "\n",
      "patient visits: \n",
      "Visit: 1  with Explanation Score: -0.031054083 [94, 362, 339, 21, 48, 8, 72, 183, 2223, 697, 5, 766, 253, 492, 1009, 67, 511, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [[362, 492, 67, 697, 2223, 183, 5, 8, 48, 21, 339, 72, 94, 253, 511, 1009, 44, 766], ['DEPRESSIVE DISORDER, NOT ELSEWHERE CLASSIFIED', 'Obstructive chronic bronchitis with (acute) exacerbation', 'UNSPECIFIED DISORDER OF KIDNEY AND URETER', 'EDEMA', 'WHEEZING', 'Personal History of Tobacco Use', 'Diabetes mellitus without mention of complication, type II or unspecified type, not stated as uncontrolled', 'OTHER AND UNSPECIFIED HYPERLIPIDEMIA', 'ANEMIA, UNSPECIFIED', 'UNSPECIFIED ESSENTIAL HYPERTENSION', 'Asthma, unspecified', 'ESOPHAGEAL REFLUX', 'OSTEOPOROSIS, UNSPECIFIED', 'SHORTNESS OF BREATH', 'Chronic obstructive asthma, with (acute) exacerbation', 'OTHER DEPENDENCE ON MACHINES, SUPPLEMENTAL OXYGEN', 'Long-Term (Current) Use of Steroids', 'Hypoxemia']] \n",
      "\n",
      "Visit: 2  with Explanation Score: 0.1121871 [21, 339, 5, 563, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [[5, 21, 339, 563], ['Diabetes mellitus without mention of complication, type II or unspecified type, not stated as uncontrolled', 'UNSPECIFIED ESSENTIAL HYPERTENSION', 'Asthma, unspecified', 'RENAL FAILURE, UNSPECIFIED']] \n",
      "\n",
      "Visit: 3  with Explanation Score: -0.070365086 [5, 67, 8, 183, 256, 21, 1050, 57, 55, 2223, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [[67, 2223, 183, 5, 8, 21, 1050, 57, 55, 256], ['UNSPECIFIED DISORDER OF KIDNEY AND URETER', 'WHEEZING', 'Personal History of Tobacco Use', 'Diabetes mellitus without mention of complication, type II or unspecified type, not stated as uncontrolled', 'OTHER AND UNSPECIFIED HYPERLIPIDEMIA', 'UNSPECIFIED ESSENTIAL HYPERTENSION', 'CHRONIC MAXILLARY SINUSITIS', 'Chronic obstructive asthma, unspecified', 'CHRONIC AIRWAY OBSTRUCTION, NOT ELSEWHERE CLASSIFIED', 'Other disease of nasal cavity and sinuses']] \n",
      "\n",
      "\u001b[1;31mHaving a score for empty visit is NOT acceptable\u001b[0m \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### predictions - Explanation for visit level\n",
    "use_cuda=True\n",
    "for c,mb in enumerate(mbs_list[-22:-20]):\n",
    "    print('mb',c)\n",
    "    x1, label,seq_len,time_diff = mb \n",
    "    x_in  = best_model.EmbedPatient_MB(x1,time_diff) \n",
    "    output, hidden = best_model.rnn_c(x_in) ## hn is the model output\n",
    "    pred = best_model.sigmoid(best_model.out(hidden[-1])).squeeze()\n",
    "    #print ('label', label.view(1,-1),'pred',pred)\n",
    "    print('outshape',output.shape)\n",
    "    x = x_in.cpu().data.numpy()\n",
    "    hn = output.cpu().data.numpy()\n",
    "    z_pdict=[]\n",
    "    mb_score_dict=[]\n",
    "    for pt in range(output.shape[0]):\n",
    "        z_dict = []\n",
    "        #z_dict.append(np.ones(150)) #### need to ask we have this\n",
    "        for i in range(output.shape[1]): ### seq lenghth\n",
    "            #i = i + 1\n",
    "            if i==0:\n",
    "                #print(type(W_iz), type(x[pt,i,:]),b_z)\n",
    "                z = sigmoid(np.matmul(W_iz, x[pt,i,:])+ b_z)\n",
    "            else:\n",
    "                z = sigmoid(np.matmul(W_iz, x[pt,i,:]) + np.matmul(W_hz, hn[pt,i-1,:]) + b_z)\n",
    "            \n",
    "            z_dict.append(z)\n",
    "        alpha_dict = z_dict\n",
    "        #print(len(alpha_dict))\n",
    "        #print(alpha_dict[0].shape)\n",
    "            \n",
    "        score_dict = []\n",
    "        for i in range(len(alpha_dict)):\n",
    "            if i == 0:\n",
    "                updating = hn[0,0,:]\n",
    "            else:\n",
    "                updating = hn[pt,i,:] - alpha_dict[i] * hn[pt,i-1,:]\n",
    "            forgetting = alpha_dict[0]\n",
    "            for j in range(i+1, len(alpha_dict)):\n",
    "                forgetting = forgetting*alpha_dict[j]\n",
    "            score = np.matmul( W[0], updating * forgetting) #+ b[target_class]\n",
    "            #print(score)\n",
    "            score_dict.append(score)\n",
    "        #print(len(score_dict))    \n",
    "            \n",
    "        z_pdict.append(z_dict)\n",
    "        mb_score_dict.append(score_dict)    \n",
    "\n",
    "    #print(len(z_pdict), len(z_pdict[-1]))\n",
    "    #print(z_pdict[0][0].shape)\n",
    "    #print (len(mb_score_dict),x1.shape,x1[-1],x_in[-1],mb_score_dict[-1])\n",
    "    print('Patient 1','label: ',label[1].cpu(), '\\npred_score :' , pred[1].cpu().data)\n",
    "          #,'\\npatient visits: ' ,(x1[1].cpu().data.numpy()).tolist(),\n",
    "    print('\\nExplanation score per visit: ',mb_score_dict[1])\n",
    "    print('\\npatient visits: ')\n",
    "    for g,l in enumerate((x1[1].cpu().data.numpy()).tolist()):\n",
    "        dls=[diag_tk[diag_tk['TOKEN_ID'].isin(l)]['TOKEN_ID'].tolist(), diag_tk[diag_tk['TOKEN_ID'].isin(l)]['DIAGNOSIS_DESCRIPTION'].tolist()]\n",
    "        print('Visit:',g+1,' with Explanation Score:',mb_score_dict[1][g],l,dls,'\\n')\n",
    "        \n",
    "    #print('\\033[1m  The above looks acceptable  \\033[0m') ### this for Terminal\n",
    "    print('\\x1b[1;31m'+'The above looks acceptable'+'\\x1b[0m')\n",
    "    \n",
    "    print('Patient -1','label: ',label[-1].cpu(), '\\npred_score :' , pred[-1].cpu().data)\n",
    "          #,'\\npatient visits: ' ,(x1[1].cpu().data.numpy()).tolist(),\n",
    "    print('\\nExplanation score per visit: ',mb_score_dict[-1])\n",
    "    print('\\npatient visits: ')\n",
    "    for g,l in enumerate((x1[-1].cpu().data.numpy()).tolist()):\n",
    "        dls=[diag_tk[diag_tk['TOKEN_ID'].isin(l)]['TOKEN_ID'].tolist(), diag_tk[diag_tk['TOKEN_ID'].isin(l)]['DIAGNOSIS_DESCRIPTION'].tolist()]\n",
    "        print('Visit:',g+1,' with Explanation Score:',mb_score_dict[-1][g],l,dls,'\\n')\n",
    "        \n",
    "    #print('\\033[1m  The above looks acceptable  \\033[0m') ### this for Terminal\n",
    "    print('\\x1b[1;31m'+'Having a score for empty visit is NOT acceptable'+'\\x1b[0m \\n\\n')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above explanations looks resonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print('label: ',label[1].cpu(), '\\npred_score :' , pred[1].cpu().data)\n",
    "          #,'\\npatient visits: ' ,(x1[1].cpu().data.numpy()).tolist(),\n",
    "    print('\\nExplanation score per visit: ',mb_score_dict[1])\n",
    "    print('\\npatient visits: ')\n",
    "    for g,l in enumerate((x1[1].cpu().data.numpy()).tolist()):\n",
    "        dls=[diag_tk[diag_tk['TOKEN_ID'].isin(l)]['TOKEN_ID'].tolist(), diag_tk[diag_tk['TOKEN_ID'].isin(l)]['DIAGNOSIS_DESCRIPTION'].tolist()]\n",
    "        print('Visit:',g+1,' with Explanation Score:',mb_score_dict[1][g],l,dls,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribution for the prediction\n",
    "\n",
    "z_dict = []\n",
    "z_dict.append(np.ones(150))\n",
    "for i in range(length-1):\n",
    "    i = i + 1\n",
    "    z = sigmoid(np.matmul(W_iz, x[i,0,:]) + np.matmul(W_hz, hn[i-1,0,:]) + b_z)\n",
    "    z_dict.append(z)\n",
    "alpha_dict = z_dict\n",
    "\n",
    "\n",
    "\n",
    "weights_linear = best_model.hidden2label.state_dict()\n",
    "W = weights_linear['weight'].cpu().numpy()\n",
    "b= weights_linear['bias'].cpu().numpy()\n",
    "\n",
    "target_class = pred_label\n",
    "score_dict = []\n",
    "for i in range(len(alpha_dict)):\n",
    "    if i == 0:\n",
    "        updating = hn[0,0,:]\n",
    "    else:\n",
    "        updating = hn[i,0,:] - alpha_dict[i] * hn[i-1,0,:]\n",
    "    forgetting = alpha_dict[0]\n",
    "    for j in range(i+1, len(alpha_dict)):\n",
    "        forgetting = forgetting*alpha_dict[j]\n",
    "    score = np.matmul( W[target_class], updating * forgetting) #+ b[target_class]\n",
    "    score_dict.append(score[0])\n",
    "return word_tokenize_list, score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/gru/best_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e638a7ea3f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m#text = \"the fight scenes are fun but it grows tedious\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"the story may be new, but it does not serve lots of laughs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/gru/best_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0mword_tokenize_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/gru/best_model.pkl'"
     ]
    }
   ],
   "source": [
    "def attribution(model,mb):\n",
    "    # data for word-to-vec\n",
    "#    text_field = data.Field(lower=True)  # it is an object\n",
    "#    label_field = data.Field(sequential=False)   # it is also an object\n",
    "#    train_iter, dev_iter, test_iter = load_sst(text_field, label_field, 5)\n",
    "    # word2vector\n",
    "#    word_to_idx = text_field.vocab.stoi  # example of word_to_idx: u'schools': 14512, len(word_to_idx) = 16190\n",
    "#    word_to_idx_dict = dict(word_to_idx)\n",
    "\n",
    "    \n",
    "    # processing text\n",
    "    word_tokenize_list = word_tokenize(text)\n",
    "    sentence = []\n",
    "    for i in word_tokenize_list:\n",
    "        sentence.append(word_to_idx_dict[i])\n",
    "    sent = np.array(sentence)\n",
    "    sent = Variable(torch.from_numpy(sent))\n",
    "    sent = sent.cuda()\n",
    "    length = len(word_tokenize_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # model prediction\n",
    "    best_model.batch_size = 1\n",
    "    best_model.hidden = best_model.init_hidden()\n",
    "    pred, hn, x = best_model(sent)\n",
    "    hn = hn.cpu().data.numpy()\n",
    "    x = x.cpu().data.numpy()\n",
    "    pred = F.softmax(pred).cpu()\n",
    "    pred_label = pred.data.max(1)[1].numpy()  \n",
    "    if pred_label[0] == 0:\n",
    "        print (\"prediction category: positive sentiment with confidence of \" + str(pred.data.numpy()[0, 0]))# 0 is positive, and 1 is negative\n",
    "    else:\n",
    "        print (\"prediction category: negative sentiment with confidence of \" + str(pred.data.numpy()[0, 1]))\n",
    "\n",
    "\n",
    "    # attribution for the prediction\n",
    "    weights = best_model.gru.state_dict()\n",
    "    _, W_iz, _ = np.split(weights['weight_ih_l0'], 3, 0)\n",
    "    _, W_hz, _ = np.split(weights['weight_hh_l0'], 3, 0)\n",
    "    _, b_z, _ = np.split(weights['bias_ih_l0'].cpu().numpy() + weights['bias_hh_l0'].cpu().numpy(), 3)\n",
    "\n",
    "    z_dict = []\n",
    "    z_dict.append(np.ones(150))\n",
    "    for i in range(length-1):\n",
    "        i = i + 1\n",
    "        z = sigmoid(np.matmul(W_iz, x[i,0,:]) + np.matmul(W_hz, hn[i-1,0,:]) + b_z)\n",
    "        z_dict.append(z)\n",
    "    alpha_dict = z_dict\n",
    "    weights_linear = best_model.hidden2label.state_dict()\n",
    "    W = weights_linear['weight'].cpu().numpy()\n",
    "    b= weights_linear['bias'].cpu().numpy()\n",
    "\n",
    "    target_class = pred_label\n",
    "    score_dict = []\n",
    "    for i in range(len(alpha_dict)):\n",
    "        if i == 0:\n",
    "            updating = hn[0,0,:]\n",
    "        else:\n",
    "            updating = hn[i,0,:] - alpha_dict[i] * hn[i-1,0,:]\n",
    "        forgetting = alpha_dict[0]\n",
    "        for j in range(i+1, len(alpha_dict)):\n",
    "            forgetting = forgetting*alpha_dict[j]\n",
    "        score = np.matmul( W[target_class], updating * forgetting) #+ b[target_class]\n",
    "        score_dict.append(score[0])\n",
    "    return word_tokenize_list, score_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# testing text\n",
    "#text = \"the fight scenes are fun but it grows tedious\"\n",
    "text = \"the story may be new, but it does not serve lots of laughs\"\n",
    "best_model = torch.load('models/gru/best_model.pkl')\n",
    "word_tokenize_list, score_dict = attribution(best_model, text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_37_env",
   "language": "python",
   "name": "py_37_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
